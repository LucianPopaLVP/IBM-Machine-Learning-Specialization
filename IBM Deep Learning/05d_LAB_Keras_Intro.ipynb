{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part d: Keras Intro LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lucya\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=660bdbd169387550dc12e2269646f9a5e577300f24463aab4b651520eeed6d88\n",
      "  Stored in directory: c:\\users\\lucya\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "from tensorflow.keras.models  import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lucya\\\\IBM-Machine-Learnin-Specialization\\\\IBM Deep Learning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set \n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv('diabetes.csv', names=names, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>178</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.415</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>10</td>\n",
       "      <td>179</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.402</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7</td>\n",
       "      <td>187</td>\n",
       "      <td>68</td>\n",
       "      <td>39</td>\n",
       "      <td>304</td>\n",
       "      <td>37.7</td>\n",
       "      <td>0.254</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.141</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "718               1                     108              60              46   \n",
       "327              10                     179              70               0   \n",
       "242               3                     139              54               0   \n",
       "56                7                     187              68              39   \n",
       "729               2                      92              52               0   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "718      178  35.5              0.415   24             0  \n",
       "327        0  35.1              0.200   37             0  \n",
       "242        0  25.6              0.402   22             1  \n",
       "56       304  37.7              0.254   41             1  \n",
       "729        0  30.1              0.141   22             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise 1: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.781\n",
      "roc-auc is 0.829\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (8,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 12)                108       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6460 - accuracy: 0.6597 - val_loss: 0.6358 - val_accuracy: 0.6927\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.6615 - val_loss: 0.6342 - val_accuracy: 0.6979\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6423 - accuracy: 0.6632 - val_loss: 0.6328 - val_accuracy: 0.6927\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.6649 - val_loss: 0.6315 - val_accuracy: 0.6927\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.6684 - val_loss: 0.6302 - val_accuracy: 0.6979\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6649 - val_loss: 0.6291 - val_accuracy: 0.6875\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.6684 - val_loss: 0.6280 - val_accuracy: 0.6823\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6350 - accuracy: 0.6701 - val_loss: 0.6270 - val_accuracy: 0.6771\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6338 - accuracy: 0.6736 - val_loss: 0.6261 - val_accuracy: 0.6771\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6326 - accuracy: 0.6684 - val_loss: 0.6252 - val_accuracy: 0.6771\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.6667 - val_loss: 0.6243 - val_accuracy: 0.6667\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.6701 - val_loss: 0.6235 - val_accuracy: 0.6667\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6296 - accuracy: 0.6684 - val_loss: 0.6228 - val_accuracy: 0.6667\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.6684 - val_loss: 0.6221 - val_accuracy: 0.6719\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6279 - accuracy: 0.6667 - val_loss: 0.6214 - val_accuracy: 0.6719\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.6667 - val_loss: 0.6207 - val_accuracy: 0.6719\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6615 - val_loss: 0.6201 - val_accuracy: 0.6667\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6254 - accuracy: 0.6615 - val_loss: 0.6195 - val_accuracy: 0.6667\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6247 - accuracy: 0.6632 - val_loss: 0.6189 - val_accuracy: 0.6719\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6240 - accuracy: 0.6632 - val_loss: 0.6183 - val_accuracy: 0.6719\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6233 - accuracy: 0.6632 - val_loss: 0.6177 - val_accuracy: 0.6719\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.6649 - val_loss: 0.6172 - val_accuracy: 0.6719\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.6615 - val_loss: 0.6166 - val_accuracy: 0.6719\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6213 - accuracy: 0.6597 - val_loss: 0.6161 - val_accuracy: 0.6719\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.6597 - val_loss: 0.6156 - val_accuracy: 0.6719\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.6580 - val_loss: 0.6150 - val_accuracy: 0.6719\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.6580 - val_loss: 0.6145 - val_accuracy: 0.6719\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.6562 - val_loss: 0.6141 - val_accuracy: 0.6719\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6580 - val_loss: 0.6136 - val_accuracy: 0.6719\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6177 - accuracy: 0.6562 - val_loss: 0.6131 - val_accuracy: 0.6667\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.6562 - val_loss: 0.6126 - val_accuracy: 0.6667\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6166 - accuracy: 0.6562 - val_loss: 0.6121 - val_accuracy: 0.6667\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.6562 - val_loss: 0.6117 - val_accuracy: 0.6667\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.6562 - val_loss: 0.6112 - val_accuracy: 0.6667\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.6562 - val_loss: 0.6107 - val_accuracy: 0.6667\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6145 - accuracy: 0.6597 - val_loss: 0.6103 - val_accuracy: 0.6719\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.6597 - val_loss: 0.6098 - val_accuracy: 0.6719\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.6597 - val_loss: 0.6094 - val_accuracy: 0.6719\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.6580 - val_loss: 0.6089 - val_accuracy: 0.6719\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6125 - accuracy: 0.6580 - val_loss: 0.6085 - val_accuracy: 0.6719\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.6580 - val_loss: 0.6080 - val_accuracy: 0.6719\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6115 - accuracy: 0.6597 - val_loss: 0.6076 - val_accuracy: 0.6719\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6110 - accuracy: 0.6580 - val_loss: 0.6071 - val_accuracy: 0.6719\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.6597 - val_loss: 0.6067 - val_accuracy: 0.6719\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.6597 - val_loss: 0.6062 - val_accuracy: 0.6719\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6615 - val_loss: 0.6058 - val_accuracy: 0.6719\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.6615 - val_loss: 0.6054 - val_accuracy: 0.6719\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.6615 - val_loss: 0.6049 - val_accuracy: 0.6719\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.6615 - val_loss: 0.6045 - val_accuracy: 0.6719\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.6615 - val_loss: 0.6041 - val_accuracy: 0.6719\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.6615 - val_loss: 0.6036 - val_accuracy: 0.6719\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.6615 - val_loss: 0.6032 - val_accuracy: 0.6719\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.6615 - val_loss: 0.6028 - val_accuracy: 0.6719\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6058 - accuracy: 0.6615 - val_loss: 0.6023 - val_accuracy: 0.6719\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6054 - accuracy: 0.6615 - val_loss: 0.6019 - val_accuracy: 0.6719\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6049 - accuracy: 0.6615 - val_loss: 0.6015 - val_accuracy: 0.6719\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6597 - val_loss: 0.6011 - val_accuracy: 0.6719\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6040 - accuracy: 0.6615 - val_loss: 0.6006 - val_accuracy: 0.6719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6035 - accuracy: 0.6649 - val_loss: 0.6002 - val_accuracy: 0.6719\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.6649 - val_loss: 0.5998 - val_accuracy: 0.6719\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6649 - val_loss: 0.5994 - val_accuracy: 0.6719\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6022 - accuracy: 0.6649 - val_loss: 0.5990 - val_accuracy: 0.6719\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 0.6649 - val_loss: 0.5985 - val_accuracy: 0.6719\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.6632 - val_loss: 0.5981 - val_accuracy: 0.6719\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.6615 - val_loss: 0.5977 - val_accuracy: 0.6719\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.6615 - val_loss: 0.5973 - val_accuracy: 0.6719\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6000 - accuracy: 0.6649 - val_loss: 0.5969 - val_accuracy: 0.6719\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5996 - accuracy: 0.6667 - val_loss: 0.5965 - val_accuracy: 0.6719\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.6667 - val_loss: 0.5960 - val_accuracy: 0.6719\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.6667 - val_loss: 0.5956 - val_accuracy: 0.6719\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6667 - val_loss: 0.5952 - val_accuracy: 0.6719\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.6667 - val_loss: 0.5948 - val_accuracy: 0.6771\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5974 - accuracy: 0.6667 - val_loss: 0.5944 - val_accuracy: 0.6771\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6684 - val_loss: 0.5940 - val_accuracy: 0.6771\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.6684 - val_loss: 0.5936 - val_accuracy: 0.6771\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.6684 - val_loss: 0.5932 - val_accuracy: 0.6771\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.6684 - val_loss: 0.5928 - val_accuracy: 0.6771\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.6684 - val_loss: 0.5924 - val_accuracy: 0.6771\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.6684 - val_loss: 0.5920 - val_accuracy: 0.6771\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5945 - accuracy: 0.6684 - val_loss: 0.5916 - val_accuracy: 0.6771\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.6684 - val_loss: 0.5912 - val_accuracy: 0.6771\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.6684 - val_loss: 0.5908 - val_accuracy: 0.6771\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5933 - accuracy: 0.6684 - val_loss: 0.5904 - val_accuracy: 0.6771\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5928 - accuracy: 0.6684 - val_loss: 0.5900 - val_accuracy: 0.6771\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6684 - val_loss: 0.5896 - val_accuracy: 0.6771\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.6684 - val_loss: 0.5892 - val_accuracy: 0.6771\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5916 - accuracy: 0.6684 - val_loss: 0.5888 - val_accuracy: 0.6771\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5912 - accuracy: 0.6684 - val_loss: 0.5884 - val_accuracy: 0.6771\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.6684 - val_loss: 0.5880 - val_accuracy: 0.6771\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5904 - accuracy: 0.6684 - val_loss: 0.5876 - val_accuracy: 0.6771\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5900 - accuracy: 0.6684 - val_loss: 0.5872 - val_accuracy: 0.6771\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.6701 - val_loss: 0.5868 - val_accuracy: 0.6771\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.6753 - val_loss: 0.5864 - val_accuracy: 0.6771\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.6753 - val_loss: 0.5861 - val_accuracy: 0.6771\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.6771 - val_loss: 0.5857 - val_accuracy: 0.6771\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.6771 - val_loss: 0.5853 - val_accuracy: 0.6823\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.6771 - val_loss: 0.5849 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.6771 - val_loss: 0.5845 - val_accuracy: 0.6927\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5868 - accuracy: 0.6771 - val_loss: 0.5841 - val_accuracy: 0.6927\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.6771 - val_loss: 0.5838 - val_accuracy: 0.6927\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6771 - val_loss: 0.5834 - val_accuracy: 0.6927\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5856 - accuracy: 0.6771 - val_loss: 0.5830 - val_accuracy: 0.6979\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.6771 - val_loss: 0.5826 - val_accuracy: 0.6979\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.6771 - val_loss: 0.5823 - val_accuracy: 0.6979\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.6771 - val_loss: 0.5819 - val_accuracy: 0.6979\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5840 - accuracy: 0.6771 - val_loss: 0.5815 - val_accuracy: 0.6979\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.6771 - val_loss: 0.5811 - val_accuracy: 0.6979\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5832 - accuracy: 0.6771 - val_loss: 0.5808 - val_accuracy: 0.6979\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.6771 - val_loss: 0.5804 - val_accuracy: 0.6979\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5825 - accuracy: 0.6788 - val_loss: 0.5800 - val_accuracy: 0.6979\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.6771 - val_loss: 0.5797 - val_accuracy: 0.6979\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.6788 - val_loss: 0.5793 - val_accuracy: 0.6979\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5813 - accuracy: 0.6788 - val_loss: 0.5789 - val_accuracy: 0.6979\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5809 - accuracy: 0.6788 - val_loss: 0.5786 - val_accuracy: 0.6979\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.6788 - val_loss: 0.5782 - val_accuracy: 0.6979\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.6806 - val_loss: 0.5778 - val_accuracy: 0.6979\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.6788 - val_loss: 0.5775 - val_accuracy: 0.6979\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.6788 - val_loss: 0.5771 - val_accuracy: 0.6979\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.6771 - val_loss: 0.5767 - val_accuracy: 0.6979\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.6771 - val_loss: 0.5764 - val_accuracy: 0.6979\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.6788 - val_loss: 0.5760 - val_accuracy: 0.6979\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.6788 - val_loss: 0.5757 - val_accuracy: 0.6979\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.6788 - val_loss: 0.5753 - val_accuracy: 0.7031\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5772 - accuracy: 0.6788 - val_loss: 0.5750 - val_accuracy: 0.7031\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.6788 - val_loss: 0.5746 - val_accuracy: 0.7031\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5764 - accuracy: 0.6788 - val_loss: 0.5743 - val_accuracy: 0.7031\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.6788 - val_loss: 0.5739 - val_accuracy: 0.7031\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5757 - accuracy: 0.6788 - val_loss: 0.5735 - val_accuracy: 0.7031\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.6788 - val_loss: 0.5732 - val_accuracy: 0.7031\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.6806 - val_loss: 0.5728 - val_accuracy: 0.7031\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5746 - accuracy: 0.6823 - val_loss: 0.5725 - val_accuracy: 0.7031\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.6806 - val_loss: 0.5722 - val_accuracy: 0.7031\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5739 - accuracy: 0.6840 - val_loss: 0.5718 - val_accuracy: 0.7031\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.6840 - val_loss: 0.5715 - val_accuracy: 0.7083\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.6840 - val_loss: 0.5711 - val_accuracy: 0.7083\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5728 - accuracy: 0.6840 - val_loss: 0.5708 - val_accuracy: 0.7135\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.6840 - val_loss: 0.5704 - val_accuracy: 0.7188\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.6840 - val_loss: 0.5701 - val_accuracy: 0.7188\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.6823 - val_loss: 0.5698 - val_accuracy: 0.7188\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.6840 - val_loss: 0.5694 - val_accuracy: 0.7188\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.6840 - val_loss: 0.5691 - val_accuracy: 0.7188\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5706 - accuracy: 0.6823 - val_loss: 0.5687 - val_accuracy: 0.7188\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.6858 - val_loss: 0.5684 - val_accuracy: 0.7188\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5700 - accuracy: 0.6858 - val_loss: 0.5681 - val_accuracy: 0.7188\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.6858 - val_loss: 0.5677 - val_accuracy: 0.7188\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.6892 - val_loss: 0.5674 - val_accuracy: 0.7240\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.6892 - val_loss: 0.5671 - val_accuracy: 0.7240\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.6892 - val_loss: 0.5667 - val_accuracy: 0.7240\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.6910 - val_loss: 0.5664 - val_accuracy: 0.7240\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5678 - accuracy: 0.6910 - val_loss: 0.5661 - val_accuracy: 0.7240\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.6910 - val_loss: 0.5658 - val_accuracy: 0.7240\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5672 - accuracy: 0.6927 - val_loss: 0.5654 - val_accuracy: 0.7240\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5668 - accuracy: 0.6944 - val_loss: 0.5651 - val_accuracy: 0.7292\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.6944 - val_loss: 0.5648 - val_accuracy: 0.7292\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.6927 - val_loss: 0.5645 - val_accuracy: 0.7292\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.6927 - val_loss: 0.5641 - val_accuracy: 0.7292\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.6927 - val_loss: 0.5638 - val_accuracy: 0.7292\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.6927 - val_loss: 0.5635 - val_accuracy: 0.7292\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.6927 - val_loss: 0.5632 - val_accuracy: 0.7292\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.6927 - val_loss: 0.5629 - val_accuracy: 0.7292\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5641 - accuracy: 0.6927 - val_loss: 0.5625 - val_accuracy: 0.7292\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.6927 - val_loss: 0.5622 - val_accuracy: 0.7344\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.6927 - val_loss: 0.5619 - val_accuracy: 0.7344\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.6927 - val_loss: 0.5616 - val_accuracy: 0.7344\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.6927 - val_loss: 0.5613 - val_accuracy: 0.7344\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.6927 - val_loss: 0.5610 - val_accuracy: 0.7344\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.6927 - val_loss: 0.5607 - val_accuracy: 0.7396\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.6927 - val_loss: 0.5603 - val_accuracy: 0.7396\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.6927 - val_loss: 0.5600 - val_accuracy: 0.7396\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5611 - accuracy: 0.6927 - val_loss: 0.5597 - val_accuracy: 0.7396\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.6927 - val_loss: 0.5594 - val_accuracy: 0.7396\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.6910 - val_loss: 0.5591 - val_accuracy: 0.7396\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.6910 - val_loss: 0.5588 - val_accuracy: 0.7448\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.6910 - val_loss: 0.5585 - val_accuracy: 0.7448\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.6910 - val_loss: 0.5582 - val_accuracy: 0.7448\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.6927 - val_loss: 0.5579 - val_accuracy: 0.7448\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.6927 - val_loss: 0.5576 - val_accuracy: 0.7448\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.6962 - val_loss: 0.5573 - val_accuracy: 0.7448\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.6962 - val_loss: 0.5570 - val_accuracy: 0.7448\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.6962 - val_loss: 0.5567 - val_accuracy: 0.7448\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.6962 - val_loss: 0.5564 - val_accuracy: 0.7448\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.6979 - val_loss: 0.5561 - val_accuracy: 0.7500\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.6979 - val_loss: 0.5558 - val_accuracy: 0.7500\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.6997 - val_loss: 0.5555 - val_accuracy: 0.7500\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.6997 - val_loss: 0.5552 - val_accuracy: 0.7500\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5559 - accuracy: 0.7014 - val_loss: 0.5549 - val_accuracy: 0.7500\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.7014 - val_loss: 0.5546 - val_accuracy: 0.7500\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5553 - accuracy: 0.7014 - val_loss: 0.5543 - val_accuracy: 0.7552\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7014 - val_loss: 0.5540 - val_accuracy: 0.7552\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5547 - accuracy: 0.7031 - val_loss: 0.5537 - val_accuracy: 0.7552\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7031 - val_loss: 0.5535 - val_accuracy: 0.7552\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5540 - accuracy: 0.7014 - val_loss: 0.5532 - val_accuracy: 0.7552\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7014 - val_loss: 0.5529 - val_accuracy: 0.7552\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5534 - accuracy: 0.7014 - val_loss: 0.5526 - val_accuracy: 0.7552\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7014 - val_loss: 0.5523 - val_accuracy: 0.7552\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7014 - val_loss: 0.5520 - val_accuracy: 0.7552\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7049 - val_loss: 0.5517 - val_accuracy: 0.7552\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.7031 - val_loss: 0.5515 - val_accuracy: 0.7552\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7083 - val_loss: 0.5512 - val_accuracy: 0.7552\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5516 - accuracy: 0.7083 - val_loss: 0.5509 - val_accuracy: 0.7552\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1600/2018128257.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#  One is a hard decision, the other is a probabilitistic score.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_pred_class_nn_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred_prob_nn_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_class_nn_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1600/1432541550.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's check out the outputs to get a feel for how keras apis work.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_class_nn_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_class_nn_1' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_prob_nn_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1600/3070183076.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred_prob_nn_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_prob_nn_1' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_class_nn_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1600/99902677.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print model performance and plot the roc curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy is {:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_class_nn_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'roc-auc is {:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_prob_nn_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_prob_nn_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_class_nn_1' is not defined"
     ]
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b25b3c3e20>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.7083 - val_loss: 0.5506 - val_accuracy: 0.7552\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7101 - val_loss: 0.5503 - val_accuracy: 0.7552\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7101 - val_loss: 0.5501 - val_accuracy: 0.7552\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7101 - val_loss: 0.5498 - val_accuracy: 0.7552\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.7101 - val_loss: 0.5495 - val_accuracy: 0.7552\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7049 - val_loss: 0.5492 - val_accuracy: 0.7552\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5495 - accuracy: 0.7101 - val_loss: 0.5490 - val_accuracy: 0.7552\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5492 - accuracy: 0.7066 - val_loss: 0.5487 - val_accuracy: 0.7552\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7066 - val_loss: 0.5484 - val_accuracy: 0.7552\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7066 - val_loss: 0.5481 - val_accuracy: 0.7552\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5483 - accuracy: 0.7066 - val_loss: 0.5479 - val_accuracy: 0.7552\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.7066 - val_loss: 0.5476 - val_accuracy: 0.7552\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7066 - val_loss: 0.5473 - val_accuracy: 0.7552\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.7066 - val_loss: 0.5471 - val_accuracy: 0.7552\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7066 - val_loss: 0.5468 - val_accuracy: 0.7552\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.7083 - val_loss: 0.5465 - val_accuracy: 0.7552\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7083 - val_loss: 0.5463 - val_accuracy: 0.7552\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7049 - val_loss: 0.5460 - val_accuracy: 0.7552\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7083 - val_loss: 0.5457 - val_accuracy: 0.7552\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7066 - val_loss: 0.5455 - val_accuracy: 0.7552\n",
      "Epoch 21/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7083 - val_loss: 0.5452 - val_accuracy: 0.7552\n",
      "Epoch 22/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7083 - val_loss: 0.5450 - val_accuracy: 0.7552\n",
      "Epoch 23/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.7101 - val_loss: 0.5447 - val_accuracy: 0.7552\n",
      "Epoch 24/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.7101 - val_loss: 0.5444 - val_accuracy: 0.7552\n",
      "Epoch 25/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7118 - val_loss: 0.5442 - val_accuracy: 0.7552\n",
      "Epoch 26/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.7135 - val_loss: 0.5439 - val_accuracy: 0.7552\n",
      "Epoch 27/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7118 - val_loss: 0.5437 - val_accuracy: 0.7552\n",
      "Epoch 28/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.7153 - val_loss: 0.5434 - val_accuracy: 0.7552\n",
      "Epoch 29/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5431 - accuracy: 0.7118 - val_loss: 0.5432 - val_accuracy: 0.7552\n",
      "Epoch 30/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.7188 - val_loss: 0.5429 - val_accuracy: 0.7552\n",
      "Epoch 31/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7170 - val_loss: 0.5427 - val_accuracy: 0.7552\n",
      "Epoch 32/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5422 - accuracy: 0.7188 - val_loss: 0.5424 - val_accuracy: 0.7552\n",
      "Epoch 33/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7205 - val_loss: 0.5422 - val_accuracy: 0.7552\n",
      "Epoch 34/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7205 - val_loss: 0.5419 - val_accuracy: 0.7604\n",
      "Epoch 35/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7205 - val_loss: 0.5417 - val_accuracy: 0.7604\n",
      "Epoch 36/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7205 - val_loss: 0.5414 - val_accuracy: 0.7604\n",
      "Epoch 37/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.7222 - val_loss: 0.5412 - val_accuracy: 0.7604\n",
      "Epoch 38/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7222 - val_loss: 0.5409 - val_accuracy: 0.7604\n",
      "Epoch 39/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7222 - val_loss: 0.5407 - val_accuracy: 0.7604\n",
      "Epoch 40/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5400 - accuracy: 0.7222 - val_loss: 0.5404 - val_accuracy: 0.7604\n",
      "Epoch 41/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7274 - val_loss: 0.5402 - val_accuracy: 0.7604\n",
      "Epoch 42/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7274 - val_loss: 0.5400 - val_accuracy: 0.7604\n",
      "Epoch 43/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7274 - val_loss: 0.5397 - val_accuracy: 0.7604\n",
      "Epoch 44/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.7274 - val_loss: 0.5395 - val_accuracy: 0.7604\n",
      "Epoch 45/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7274 - val_loss: 0.5392 - val_accuracy: 0.7604\n",
      "Epoch 46/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7292 - val_loss: 0.5390 - val_accuracy: 0.7604\n",
      "Epoch 47/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7292 - val_loss: 0.5388 - val_accuracy: 0.7604\n",
      "Epoch 48/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 0.7274 - val_loss: 0.5385 - val_accuracy: 0.7604\n",
      "Epoch 49/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.7274 - val_loss: 0.5383 - val_accuracy: 0.7552\n",
      "Epoch 50/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7292 - val_loss: 0.5380 - val_accuracy: 0.7552\n",
      "Epoch 51/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7292 - val_loss: 0.5378 - val_accuracy: 0.7552\n",
      "Epoch 52/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7292 - val_loss: 0.5376 - val_accuracy: 0.7604\n",
      "Epoch 53/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7292 - val_loss: 0.5373 - val_accuracy: 0.7604\n",
      "Epoch 54/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7309 - val_loss: 0.5371 - val_accuracy: 0.7604\n",
      "Epoch 55/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7309 - val_loss: 0.5369 - val_accuracy: 0.7604\n",
      "Epoch 56/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7309 - val_loss: 0.5366 - val_accuracy: 0.7604\n",
      "Epoch 57/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7309 - val_loss: 0.5364 - val_accuracy: 0.7604\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7309 - val_loss: 0.5362 - val_accuracy: 0.7604\n",
      "Epoch 59/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7309 - val_loss: 0.5360 - val_accuracy: 0.7604\n",
      "Epoch 60/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7309 - val_loss: 0.5357 - val_accuracy: 0.7604\n",
      "Epoch 61/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7326 - val_loss: 0.5355 - val_accuracy: 0.7604\n",
      "Epoch 62/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7344 - val_loss: 0.5353 - val_accuracy: 0.7604\n",
      "Epoch 63/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7344 - val_loss: 0.5351 - val_accuracy: 0.7604\n",
      "Epoch 64/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7344 - val_loss: 0.5348 - val_accuracy: 0.7604\n",
      "Epoch 65/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.7344 - val_loss: 0.5346 - val_accuracy: 0.7604\n",
      "Epoch 66/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7344 - val_loss: 0.5344 - val_accuracy: 0.7604\n",
      "Epoch 67/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7344 - val_loss: 0.5342 - val_accuracy: 0.7604\n",
      "Epoch 68/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5327 - accuracy: 0.7361 - val_loss: 0.5340 - val_accuracy: 0.7604\n",
      "Epoch 69/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7361 - val_loss: 0.5337 - val_accuracy: 0.7552\n",
      "Epoch 70/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7361 - val_loss: 0.5335 - val_accuracy: 0.7604\n",
      "Epoch 71/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7361 - val_loss: 0.5333 - val_accuracy: 0.7604\n",
      "Epoch 72/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7361 - val_loss: 0.5331 - val_accuracy: 0.7604\n",
      "Epoch 73/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7326 - val_loss: 0.5329 - val_accuracy: 0.7604\n",
      "Epoch 74/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5312 - accuracy: 0.7326 - val_loss: 0.5326 - val_accuracy: 0.7604\n",
      "Epoch 75/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.7309 - val_loss: 0.5324 - val_accuracy: 0.7604\n",
      "Epoch 76/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7292 - val_loss: 0.5322 - val_accuracy: 0.7604\n",
      "Epoch 77/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7292 - val_loss: 0.5320 - val_accuracy: 0.7604\n",
      "Epoch 78/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7292 - val_loss: 0.5318 - val_accuracy: 0.7604\n",
      "Epoch 79/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7292 - val_loss: 0.5316 - val_accuracy: 0.7604\n",
      "Epoch 80/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7292 - val_loss: 0.5314 - val_accuracy: 0.7604\n",
      "Epoch 81/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7274 - val_loss: 0.5312 - val_accuracy: 0.7604\n",
      "Epoch 82/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7257 - val_loss: 0.5310 - val_accuracy: 0.7604\n",
      "Epoch 83/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7274 - val_loss: 0.5308 - val_accuracy: 0.7604\n",
      "Epoch 84/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7257 - val_loss: 0.5305 - val_accuracy: 0.7604\n",
      "Epoch 85/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7274 - val_loss: 0.5303 - val_accuracy: 0.7604\n",
      "Epoch 86/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7274 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 87/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7292 - val_loss: 0.5299 - val_accuracy: 0.7656\n",
      "Epoch 88/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7292 - val_loss: 0.5297 - val_accuracy: 0.7656\n",
      "Epoch 89/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7292 - val_loss: 0.5295 - val_accuracy: 0.7656\n",
      "Epoch 90/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7292 - val_loss: 0.5293 - val_accuracy: 0.7656\n",
      "Epoch 91/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7292 - val_loss: 0.5291 - val_accuracy: 0.7604\n",
      "Epoch 92/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.7309 - val_loss: 0.5289 - val_accuracy: 0.7604\n",
      "Epoch 93/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7274 - val_loss: 0.5287 - val_accuracy: 0.7604\n",
      "Epoch 94/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7292 - val_loss: 0.5285 - val_accuracy: 0.7604\n",
      "Epoch 95/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7292 - val_loss: 0.5283 - val_accuracy: 0.7604\n",
      "Epoch 96/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7292 - val_loss: 0.5281 - val_accuracy: 0.7604\n",
      "Epoch 97/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7309 - val_loss: 0.5279 - val_accuracy: 0.7604\n",
      "Epoch 98/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7309 - val_loss: 0.5277 - val_accuracy: 0.7604\n",
      "Epoch 99/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7309 - val_loss: 0.5275 - val_accuracy: 0.7604\n",
      "Epoch 100/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.7309 - val_loss: 0.5273 - val_accuracy: 0.7604\n",
      "Epoch 101/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7309 - val_loss: 0.5271 - val_accuracy: 0.7604\n",
      "Epoch 102/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5245 - accuracy: 0.7309 - val_loss: 0.5269 - val_accuracy: 0.7604\n",
      "Epoch 103/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7309 - val_loss: 0.5268 - val_accuracy: 0.7604\n",
      "Epoch 104/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.7309 - val_loss: 0.5266 - val_accuracy: 0.7604\n",
      "Epoch 105/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7309 - val_loss: 0.5264 - val_accuracy: 0.7604\n",
      "Epoch 106/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7326 - val_loss: 0.5262 - val_accuracy: 0.7604\n",
      "Epoch 107/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7344 - val_loss: 0.5260 - val_accuracy: 0.7604\n",
      "Epoch 108/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7378 - val_loss: 0.5258 - val_accuracy: 0.7604\n",
      "Epoch 109/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7378 - val_loss: 0.5256 - val_accuracy: 0.7604\n",
      "Epoch 110/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7378 - val_loss: 0.5254 - val_accuracy: 0.7604\n",
      "Epoch 111/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7378 - val_loss: 0.5252 - val_accuracy: 0.7656\n",
      "Epoch 112/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7396 - val_loss: 0.5251 - val_accuracy: 0.7708\n",
      "Epoch 113/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.7396 - val_loss: 0.5249 - val_accuracy: 0.7760\n",
      "Epoch 114/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7396 - val_loss: 0.5247 - val_accuracy: 0.7760\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7396 - val_loss: 0.5245 - val_accuracy: 0.7760\n",
      "Epoch 116/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7413 - val_loss: 0.5243 - val_accuracy: 0.7760\n",
      "Epoch 117/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7413 - val_loss: 0.5241 - val_accuracy: 0.7760\n",
      "Epoch 118/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.7413 - val_loss: 0.5240 - val_accuracy: 0.7760\n",
      "Epoch 119/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7413 - val_loss: 0.5238 - val_accuracy: 0.7760\n",
      "Epoch 120/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7413 - val_loss: 0.5236 - val_accuracy: 0.7760\n",
      "Epoch 121/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7431 - val_loss: 0.5234 - val_accuracy: 0.7812\n",
      "Epoch 122/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.7413 - val_loss: 0.5232 - val_accuracy: 0.7812\n",
      "Epoch 123/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7431 - val_loss: 0.5231 - val_accuracy: 0.7812\n",
      "Epoch 124/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.7431 - val_loss: 0.5229 - val_accuracy: 0.7812\n",
      "Epoch 125/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7448 - val_loss: 0.5227 - val_accuracy: 0.7812\n",
      "Epoch 126/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7465 - val_loss: 0.5225 - val_accuracy: 0.7812\n",
      "Epoch 127/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7431 - val_loss: 0.5224 - val_accuracy: 0.7812\n",
      "Epoch 128/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7465 - val_loss: 0.5222 - val_accuracy: 0.7812\n",
      "Epoch 129/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7465 - val_loss: 0.5220 - val_accuracy: 0.7812\n",
      "Epoch 130/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7465 - val_loss: 0.5219 - val_accuracy: 0.7812\n",
      "Epoch 131/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7465 - val_loss: 0.5217 - val_accuracy: 0.7812\n",
      "Epoch 132/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7465 - val_loss: 0.5215 - val_accuracy: 0.7812\n",
      "Epoch 133/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7465 - val_loss: 0.5213 - val_accuracy: 0.7812\n",
      "Epoch 134/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7465 - val_loss: 0.5212 - val_accuracy: 0.7812\n",
      "Epoch 135/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7465 - val_loss: 0.5210 - val_accuracy: 0.7812\n",
      "Epoch 136/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7465 - val_loss: 0.5208 - val_accuracy: 0.7812\n",
      "Epoch 137/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7465 - val_loss: 0.5207 - val_accuracy: 0.7812\n",
      "Epoch 138/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7465 - val_loss: 0.5205 - val_accuracy: 0.7812\n",
      "Epoch 139/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7465 - val_loss: 0.5203 - val_accuracy: 0.7812\n",
      "Epoch 140/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7465 - val_loss: 0.5202 - val_accuracy: 0.7812\n",
      "Epoch 141/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7465 - val_loss: 0.5200 - val_accuracy: 0.7812\n",
      "Epoch 142/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7465 - val_loss: 0.5198 - val_accuracy: 0.7812\n",
      "Epoch 143/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7465 - val_loss: 0.5197 - val_accuracy: 0.7812\n",
      "Epoch 144/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7465 - val_loss: 0.5195 - val_accuracy: 0.7812\n",
      "Epoch 145/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7465 - val_loss: 0.5194 - val_accuracy: 0.7812\n",
      "Epoch 146/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7465 - val_loss: 0.5192 - val_accuracy: 0.7812\n",
      "Epoch 147/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7465 - val_loss: 0.5190 - val_accuracy: 0.7812\n",
      "Epoch 148/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.7465 - val_loss: 0.5189 - val_accuracy: 0.7812\n",
      "Epoch 149/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7465 - val_loss: 0.5187 - val_accuracy: 0.7812\n",
      "Epoch 150/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7465 - val_loss: 0.5186 - val_accuracy: 0.7812\n",
      "Epoch 151/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7465 - val_loss: 0.5184 - val_accuracy: 0.7812\n",
      "Epoch 152/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7465 - val_loss: 0.5182 - val_accuracy: 0.7812\n",
      "Epoch 153/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7465 - val_loss: 0.5181 - val_accuracy: 0.7812\n",
      "Epoch 154/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7483 - val_loss: 0.5179 - val_accuracy: 0.7812\n",
      "Epoch 155/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7483 - val_loss: 0.5178 - val_accuracy: 0.7812\n",
      "Epoch 156/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7483 - val_loss: 0.5176 - val_accuracy: 0.7812\n",
      "Epoch 157/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7483 - val_loss: 0.5175 - val_accuracy: 0.7812\n",
      "Epoch 158/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7483 - val_loss: 0.5173 - val_accuracy: 0.7812\n",
      "Epoch 159/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7465 - val_loss: 0.5172 - val_accuracy: 0.7812\n",
      "Epoch 160/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7483 - val_loss: 0.5170 - val_accuracy: 0.7812\n",
      "Epoch 161/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7465 - val_loss: 0.5169 - val_accuracy: 0.7812\n",
      "Epoch 162/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7465 - val_loss: 0.5167 - val_accuracy: 0.7812\n",
      "Epoch 163/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7483 - val_loss: 0.5166 - val_accuracy: 0.7812\n",
      "Epoch 164/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7483 - val_loss: 0.5164 - val_accuracy: 0.7812\n",
      "Epoch 165/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7465 - val_loss: 0.5163 - val_accuracy: 0.7812\n",
      "Epoch 166/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7465 - val_loss: 0.5161 - val_accuracy: 0.7812\n",
      "Epoch 167/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7465 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 168/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7465 - val_loss: 0.5158 - val_accuracy: 0.7760\n",
      "Epoch 169/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7465 - val_loss: 0.5157 - val_accuracy: 0.7760\n",
      "Epoch 170/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7483 - val_loss: 0.5155 - val_accuracy: 0.7708\n",
      "Epoch 171/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7483 - val_loss: 0.5154 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7483 - val_loss: 0.5152 - val_accuracy: 0.7708\n",
      "Epoch 173/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7500 - val_loss: 0.5151 - val_accuracy: 0.7708\n",
      "Epoch 174/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7500 - val_loss: 0.5150 - val_accuracy: 0.7760\n",
      "Epoch 175/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7483 - val_loss: 0.5148 - val_accuracy: 0.7760\n",
      "Epoch 176/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7500 - val_loss: 0.5147 - val_accuracy: 0.7760\n",
      "Epoch 177/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7483 - val_loss: 0.5145 - val_accuracy: 0.7760\n",
      "Epoch 178/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7465 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 179/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7465 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 180/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7483 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 181/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7465 - val_loss: 0.5140 - val_accuracy: 0.7760\n",
      "Epoch 182/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7465 - val_loss: 0.5138 - val_accuracy: 0.7760\n",
      "Epoch 183/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7465 - val_loss: 0.5137 - val_accuracy: 0.7760\n",
      "Epoch 184/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7465 - val_loss: 0.5136 - val_accuracy: 0.7760\n",
      "Epoch 185/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.7465 - val_loss: 0.5134 - val_accuracy: 0.7760\n",
      "Epoch 186/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7465 - val_loss: 0.5133 - val_accuracy: 0.7708\n",
      "Epoch 187/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7465 - val_loss: 0.5132 - val_accuracy: 0.7708\n",
      "Epoch 188/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7465 - val_loss: 0.5130 - val_accuracy: 0.7708\n",
      "Epoch 189/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7483 - val_loss: 0.5129 - val_accuracy: 0.7708\n",
      "Epoch 190/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7483 - val_loss: 0.5128 - val_accuracy: 0.7708\n",
      "Epoch 191/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7483 - val_loss: 0.5126 - val_accuracy: 0.7708\n",
      "Epoch 192/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7483 - val_loss: 0.5125 - val_accuracy: 0.7708\n",
      "Epoch 193/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7483 - val_loss: 0.5124 - val_accuracy: 0.7708\n",
      "Epoch 194/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7483 - val_loss: 0.5122 - val_accuracy: 0.7708\n",
      "Epoch 195/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7500 - val_loss: 0.5121 - val_accuracy: 0.7708\n",
      "Epoch 196/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7483 - val_loss: 0.5120 - val_accuracy: 0.7708\n",
      "Epoch 197/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.7500 - val_loss: 0.5118 - val_accuracy: 0.7708\n",
      "Epoch 198/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5056 - accuracy: 0.7500 - val_loss: 0.5117 - val_accuracy: 0.7708\n",
      "Epoch 199/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7500 - val_loss: 0.5116 - val_accuracy: 0.7708\n",
      "Epoch 200/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.7500 - val_loss: 0.5115 - val_accuracy: 0.7708\n",
      "Epoch 201/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7500 - val_loss: 0.5113 - val_accuracy: 0.7708\n",
      "Epoch 202/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7500 - val_loss: 0.5112 - val_accuracy: 0.7708\n",
      "Epoch 203/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7535 - val_loss: 0.5111 - val_accuracy: 0.7708\n",
      "Epoch 204/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7517 - val_loss: 0.5110 - val_accuracy: 0.7708\n",
      "Epoch 205/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7535 - val_loss: 0.5108 - val_accuracy: 0.7708\n",
      "Epoch 206/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7517 - val_loss: 0.5107 - val_accuracy: 0.7708\n",
      "Epoch 207/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7517 - val_loss: 0.5106 - val_accuracy: 0.7708\n",
      "Epoch 208/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7535 - val_loss: 0.5105 - val_accuracy: 0.7708\n",
      "Epoch 209/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.7535 - val_loss: 0.5103 - val_accuracy: 0.7708\n",
      "Epoch 210/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7552 - val_loss: 0.5102 - val_accuracy: 0.7708\n",
      "Epoch 211/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7569 - val_loss: 0.5101 - val_accuracy: 0.7708\n",
      "Epoch 212/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5033 - accuracy: 0.7569 - val_loss: 0.5100 - val_accuracy: 0.7708\n",
      "Epoch 213/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7587 - val_loss: 0.5099 - val_accuracy: 0.7708\n",
      "Epoch 214/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7587 - val_loss: 0.5097 - val_accuracy: 0.7708\n",
      "Epoch 215/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7587 - val_loss: 0.5096 - val_accuracy: 0.7708\n",
      "Epoch 216/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7587 - val_loss: 0.5095 - val_accuracy: 0.7708\n",
      "Epoch 217/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7587 - val_loss: 0.5094 - val_accuracy: 0.7708\n",
      "Epoch 218/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7587 - val_loss: 0.5093 - val_accuracy: 0.7760\n",
      "Epoch 219/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7587 - val_loss: 0.5092 - val_accuracy: 0.7760\n",
      "Epoch 220/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7587 - val_loss: 0.5090 - val_accuracy: 0.7760\n",
      "Epoch 221/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5018 - accuracy: 0.7587 - val_loss: 0.5089 - val_accuracy: 0.7760\n",
      "Epoch 222/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7587 - val_loss: 0.5088 - val_accuracy: 0.7760\n",
      "Epoch 223/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7587 - val_loss: 0.5087 - val_accuracy: 0.7760\n",
      "Epoch 224/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7587 - val_loss: 0.5086 - val_accuracy: 0.7760\n",
      "Epoch 225/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7587 - val_loss: 0.5085 - val_accuracy: 0.7760\n",
      "Epoch 226/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7587 - val_loss: 0.5083 - val_accuracy: 0.7760\n",
      "Epoch 227/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7587 - val_loss: 0.5082 - val_accuracy: 0.7760\n",
      "Epoch 228/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7587 - val_loss: 0.5081 - val_accuracy: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7569 - val_loss: 0.5080 - val_accuracy: 0.7760\n",
      "Epoch 230/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.7569 - val_loss: 0.5079 - val_accuracy: 0.7760\n",
      "Epoch 231/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7569 - val_loss: 0.5078 - val_accuracy: 0.7760\n",
      "Epoch 232/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7569 - val_loss: 0.5077 - val_accuracy: 0.7760\n",
      "Epoch 233/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.7569 - val_loss: 0.5076 - val_accuracy: 0.7760\n",
      "Epoch 234/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7569 - val_loss: 0.5075 - val_accuracy: 0.7760\n",
      "Epoch 235/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7569 - val_loss: 0.5074 - val_accuracy: 0.7760\n",
      "Epoch 236/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7569 - val_loss: 0.5072 - val_accuracy: 0.7760\n",
      "Epoch 237/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7569 - val_loss: 0.5071 - val_accuracy: 0.7760\n",
      "Epoch 238/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7569 - val_loss: 0.5070 - val_accuracy: 0.7760\n",
      "Epoch 239/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7569 - val_loss: 0.5069 - val_accuracy: 0.7812\n",
      "Epoch 240/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.7587 - val_loss: 0.5068 - val_accuracy: 0.7812\n",
      "Epoch 241/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.7587 - val_loss: 0.5067 - val_accuracy: 0.7812\n",
      "Epoch 242/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7587 - val_loss: 0.5066 - val_accuracy: 0.7812\n",
      "Epoch 243/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7587 - val_loss: 0.5065 - val_accuracy: 0.7812\n",
      "Epoch 244/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7587 - val_loss: 0.5064 - val_accuracy: 0.7812\n",
      "Epoch 245/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.7587 - val_loss: 0.5063 - val_accuracy: 0.7812\n",
      "Epoch 246/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7587 - val_loss: 0.5062 - val_accuracy: 0.7812\n",
      "Epoch 247/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7587 - val_loss: 0.5061 - val_accuracy: 0.7865\n",
      "Epoch 248/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7587 - val_loss: 0.5060 - val_accuracy: 0.7812\n",
      "Epoch 249/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7587 - val_loss: 0.5059 - val_accuracy: 0.7812\n",
      "Epoch 250/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7587 - val_loss: 0.5058 - val_accuracy: 0.7812\n",
      "Epoch 251/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7587 - val_loss: 0.5057 - val_accuracy: 0.7812\n",
      "Epoch 252/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7587 - val_loss: 0.5056 - val_accuracy: 0.7760\n",
      "Epoch 253/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.7587 - val_loss: 0.5055 - val_accuracy: 0.7760\n",
      "Epoch 254/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7587 - val_loss: 0.5054 - val_accuracy: 0.7760\n",
      "Epoch 255/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.7587 - val_loss: 0.5053 - val_accuracy: 0.7760\n",
      "Epoch 256/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7587 - val_loss: 0.5052 - val_accuracy: 0.7760\n",
      "Epoch 257/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7587 - val_loss: 0.5051 - val_accuracy: 0.7760\n",
      "Epoch 258/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7587 - val_loss: 0.5050 - val_accuracy: 0.7760\n",
      "Epoch 259/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.7587 - val_loss: 0.5049 - val_accuracy: 0.7760\n",
      "Epoch 260/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7587 - val_loss: 0.5048 - val_accuracy: 0.7760\n",
      "Epoch 261/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7587 - val_loss: 0.5047 - val_accuracy: 0.7760\n",
      "Epoch 262/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7587 - val_loss: 0.5046 - val_accuracy: 0.7760\n",
      "Epoch 263/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7587 - val_loss: 0.5045 - val_accuracy: 0.7760\n",
      "Epoch 264/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.7604 - val_loss: 0.5044 - val_accuracy: 0.7760\n",
      "Epoch 265/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.7604 - val_loss: 0.5043 - val_accuracy: 0.7760\n",
      "Epoch 266/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7604 - val_loss: 0.5042 - val_accuracy: 0.7760\n",
      "Epoch 267/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.7604 - val_loss: 0.5041 - val_accuracy: 0.7760\n",
      "Epoch 268/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7604 - val_loss: 0.5040 - val_accuracy: 0.7760\n",
      "Epoch 269/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.7604 - val_loss: 0.5039 - val_accuracy: 0.7760\n",
      "Epoch 270/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.7587 - val_loss: 0.5039 - val_accuracy: 0.7760\n",
      "Epoch 271/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7587 - val_loss: 0.5038 - val_accuracy: 0.7760\n",
      "Epoch 272/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.7587 - val_loss: 0.5037 - val_accuracy: 0.7760\n",
      "Epoch 273/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7587 - val_loss: 0.5036 - val_accuracy: 0.7760\n",
      "Epoch 274/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7604 - val_loss: 0.5035 - val_accuracy: 0.7760\n",
      "Epoch 275/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7604 - val_loss: 0.5034 - val_accuracy: 0.7760\n",
      "Epoch 276/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7604 - val_loss: 0.5033 - val_accuracy: 0.7760\n",
      "Epoch 277/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7604 - val_loss: 0.5032 - val_accuracy: 0.7760\n",
      "Epoch 278/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7622 - val_loss: 0.5031 - val_accuracy: 0.7760\n",
      "Epoch 279/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7622 - val_loss: 0.5030 - val_accuracy: 0.7760\n",
      "Epoch 280/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.7622 - val_loss: 0.5030 - val_accuracy: 0.7760\n",
      "Epoch 281/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7622 - val_loss: 0.5029 - val_accuracy: 0.7708\n",
      "Epoch 282/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7622 - val_loss: 0.5028 - val_accuracy: 0.7708\n",
      "Epoch 283/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7622 - val_loss: 0.5027 - val_accuracy: 0.7708\n",
      "Epoch 284/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.7622 - val_loss: 0.5026 - val_accuracy: 0.7708\n",
      "Epoch 285/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7622 - val_loss: 0.5025 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.7622 - val_loss: 0.5024 - val_accuracy: 0.7708\n",
      "Epoch 287/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7622 - val_loss: 0.5024 - val_accuracy: 0.7708\n",
      "Epoch 288/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7622 - val_loss: 0.5023 - val_accuracy: 0.7708\n",
      "Epoch 289/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.7622 - val_loss: 0.5022 - val_accuracy: 0.7708\n",
      "Epoch 290/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7622 - val_loss: 0.5021 - val_accuracy: 0.7708\n",
      "Epoch 291/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.7622 - val_loss: 0.5020 - val_accuracy: 0.7708\n",
      "Epoch 292/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7622 - val_loss: 0.5019 - val_accuracy: 0.7708\n",
      "Epoch 293/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7622 - val_loss: 0.5019 - val_accuracy: 0.7708\n",
      "Epoch 294/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.7622 - val_loss: 0.5018 - val_accuracy: 0.7708\n",
      "Epoch 295/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7622 - val_loss: 0.5017 - val_accuracy: 0.7708\n",
      "Epoch 296/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.7622 - val_loss: 0.5016 - val_accuracy: 0.7708\n",
      "Epoch 297/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7622 - val_loss: 0.5015 - val_accuracy: 0.7708\n",
      "Epoch 298/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7622 - val_loss: 0.5014 - val_accuracy: 0.7708\n",
      "Epoch 299/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7622 - val_loss: 0.5014 - val_accuracy: 0.7708\n",
      "Epoch 300/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7622 - val_loss: 0.5013 - val_accuracy: 0.7708\n",
      "Epoch 301/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7622 - val_loss: 0.5012 - val_accuracy: 0.7708\n",
      "Epoch 302/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7622 - val_loss: 0.5011 - val_accuracy: 0.7708\n",
      "Epoch 303/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7622 - val_loss: 0.5010 - val_accuracy: 0.7708\n",
      "Epoch 304/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.7622 - val_loss: 0.5010 - val_accuracy: 0.7708\n",
      "Epoch 305/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7622 - val_loss: 0.5009 - val_accuracy: 0.7708\n",
      "Epoch 306/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.7622 - val_loss: 0.5008 - val_accuracy: 0.7708\n",
      "Epoch 307/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7622 - val_loss: 0.5007 - val_accuracy: 0.7708\n",
      "Epoch 308/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.7622 - val_loss: 0.5007 - val_accuracy: 0.7708\n",
      "Epoch 309/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.7639 - val_loss: 0.5006 - val_accuracy: 0.7708\n",
      "Epoch 310/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7639 - val_loss: 0.5005 - val_accuracy: 0.7708\n",
      "Epoch 311/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.7639 - val_loss: 0.5004 - val_accuracy: 0.7708\n",
      "Epoch 312/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7639 - val_loss: 0.5004 - val_accuracy: 0.7708\n",
      "Epoch 313/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7639 - val_loss: 0.5003 - val_accuracy: 0.7708\n",
      "Epoch 314/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7639 - val_loss: 0.5002 - val_accuracy: 0.7708\n",
      "Epoch 315/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7639 - val_loss: 0.5001 - val_accuracy: 0.7708\n",
      "Epoch 316/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7639 - val_loss: 0.5001 - val_accuracy: 0.7708\n",
      "Epoch 317/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7639 - val_loss: 0.5000 - val_accuracy: 0.7708\n",
      "Epoch 318/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7639 - val_loss: 0.4999 - val_accuracy: 0.7708\n",
      "Epoch 319/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.7639 - val_loss: 0.4998 - val_accuracy: 0.7708\n",
      "Epoch 320/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.7639 - val_loss: 0.4998 - val_accuracy: 0.7708\n",
      "Epoch 321/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7639 - val_loss: 0.4997 - val_accuracy: 0.7708\n",
      "Epoch 322/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7639 - val_loss: 0.4996 - val_accuracy: 0.7708\n",
      "Epoch 323/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.7639 - val_loss: 0.4995 - val_accuracy: 0.7708\n",
      "Epoch 324/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7639 - val_loss: 0.4995 - val_accuracy: 0.7708\n",
      "Epoch 325/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7639 - val_loss: 0.4994 - val_accuracy: 0.7708\n",
      "Epoch 326/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7656 - val_loss: 0.4993 - val_accuracy: 0.7708\n",
      "Epoch 327/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7656 - val_loss: 0.4993 - val_accuracy: 0.7708\n",
      "Epoch 328/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7656 - val_loss: 0.4992 - val_accuracy: 0.7708\n",
      "Epoch 329/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7656 - val_loss: 0.4991 - val_accuracy: 0.7760\n",
      "Epoch 330/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.7656 - val_loss: 0.4991 - val_accuracy: 0.7760\n",
      "Epoch 331/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7656 - val_loss: 0.4990 - val_accuracy: 0.7760\n",
      "Epoch 332/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7656 - val_loss: 0.4989 - val_accuracy: 0.7760\n",
      "Epoch 333/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.7656 - val_loss: 0.4988 - val_accuracy: 0.7708\n",
      "Epoch 334/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7656 - val_loss: 0.4988 - val_accuracy: 0.7708\n",
      "Epoch 335/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7656 - val_loss: 0.4987 - val_accuracy: 0.7708\n",
      "Epoch 336/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7656 - val_loss: 0.4986 - val_accuracy: 0.7708\n",
      "Epoch 337/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7656 - val_loss: 0.4986 - val_accuracy: 0.7708\n",
      "Epoch 338/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.7656 - val_loss: 0.4985 - val_accuracy: 0.7708\n",
      "Epoch 339/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7656 - val_loss: 0.4984 - val_accuracy: 0.7708\n",
      "Epoch 340/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.7656 - val_loss: 0.4984 - val_accuracy: 0.7708\n",
      "Epoch 341/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.7656 - val_loss: 0.4983 - val_accuracy: 0.7708\n",
      "Epoch 342/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7656 - val_loss: 0.4983 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7674 - val_loss: 0.4982 - val_accuracy: 0.7708\n",
      "Epoch 344/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7656 - val_loss: 0.4981 - val_accuracy: 0.7708\n",
      "Epoch 345/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7674 - val_loss: 0.4981 - val_accuracy: 0.7708\n",
      "Epoch 346/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7674 - val_loss: 0.4980 - val_accuracy: 0.7708\n",
      "Epoch 347/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7674 - val_loss: 0.4979 - val_accuracy: 0.7708\n",
      "Epoch 348/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7674 - val_loss: 0.4979 - val_accuracy: 0.7708\n",
      "Epoch 349/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7674 - val_loss: 0.4978 - val_accuracy: 0.7708\n",
      "Epoch 350/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7674 - val_loss: 0.4977 - val_accuracy: 0.7708\n",
      "Epoch 351/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7691 - val_loss: 0.4977 - val_accuracy: 0.7708\n",
      "Epoch 352/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7691 - val_loss: 0.4976 - val_accuracy: 0.7708\n",
      "Epoch 353/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7691 - val_loss: 0.4976 - val_accuracy: 0.7708\n",
      "Epoch 354/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7691 - val_loss: 0.4975 - val_accuracy: 0.7708\n",
      "Epoch 355/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7691 - val_loss: 0.4974 - val_accuracy: 0.7708\n",
      "Epoch 356/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.7691 - val_loss: 0.4974 - val_accuracy: 0.7708\n",
      "Epoch 357/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.7691 - val_loss: 0.4973 - val_accuracy: 0.7708\n",
      "Epoch 358/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.7691 - val_loss: 0.4972 - val_accuracy: 0.7708\n",
      "Epoch 359/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.7691 - val_loss: 0.4972 - val_accuracy: 0.7708\n",
      "Epoch 360/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7691 - val_loss: 0.4971 - val_accuracy: 0.7708\n",
      "Epoch 361/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7691 - val_loss: 0.4971 - val_accuracy: 0.7708\n",
      "Epoch 362/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7691 - val_loss: 0.4970 - val_accuracy: 0.7708\n",
      "Epoch 363/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7691 - val_loss: 0.4970 - val_accuracy: 0.7656\n",
      "Epoch 364/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4840 - accuracy: 0.7691 - val_loss: 0.4969 - val_accuracy: 0.7656\n",
      "Epoch 365/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7691 - val_loss: 0.4968 - val_accuracy: 0.7656\n",
      "Epoch 366/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7691 - val_loss: 0.4968 - val_accuracy: 0.7656\n",
      "Epoch 367/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7691 - val_loss: 0.4967 - val_accuracy: 0.7656\n",
      "Epoch 368/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7691 - val_loss: 0.4967 - val_accuracy: 0.7656\n",
      "Epoch 369/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.7691 - val_loss: 0.4966 - val_accuracy: 0.7656\n",
      "Epoch 370/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7691 - val_loss: 0.4965 - val_accuracy: 0.7656\n",
      "Epoch 371/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7691 - val_loss: 0.4965 - val_accuracy: 0.7656\n",
      "Epoch 372/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7691 - val_loss: 0.4964 - val_accuracy: 0.7656\n",
      "Epoch 373/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4832 - accuracy: 0.7691 - val_loss: 0.4964 - val_accuracy: 0.7656\n",
      "Epoch 374/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7691 - val_loss: 0.4963 - val_accuracy: 0.7656\n",
      "Epoch 375/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7691 - val_loss: 0.4963 - val_accuracy: 0.7656\n",
      "Epoch 376/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7691 - val_loss: 0.4962 - val_accuracy: 0.7656\n",
      "Epoch 377/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7691 - val_loss: 0.4962 - val_accuracy: 0.7656\n",
      "Epoch 378/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7691 - val_loss: 0.4961 - val_accuracy: 0.7656\n",
      "Epoch 379/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7691 - val_loss: 0.4960 - val_accuracy: 0.7656\n",
      "Epoch 380/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7691 - val_loss: 0.4960 - val_accuracy: 0.7656\n",
      "Epoch 381/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7691 - val_loss: 0.4959 - val_accuracy: 0.7656\n",
      "Epoch 382/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7691 - val_loss: 0.4959 - val_accuracy: 0.7656\n",
      "Epoch 383/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7708 - val_loss: 0.4958 - val_accuracy: 0.7656\n",
      "Epoch 384/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.7691 - val_loss: 0.4958 - val_accuracy: 0.7656\n",
      "Epoch 385/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7708 - val_loss: 0.4957 - val_accuracy: 0.7656\n",
      "Epoch 386/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7691 - val_loss: 0.4957 - val_accuracy: 0.7656\n",
      "Epoch 387/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7708 - val_loss: 0.4956 - val_accuracy: 0.7656\n",
      "Epoch 388/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7708 - val_loss: 0.4956 - val_accuracy: 0.7656\n",
      "Epoch 389/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.7708 - val_loss: 0.4955 - val_accuracy: 0.7656\n",
      "Epoch 390/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7708 - val_loss: 0.4955 - val_accuracy: 0.7656\n",
      "Epoch 391/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7708 - val_loss: 0.4954 - val_accuracy: 0.7656\n",
      "Epoch 392/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7708 - val_loss: 0.4954 - val_accuracy: 0.7656\n",
      "Epoch 393/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.7708 - val_loss: 0.4953 - val_accuracy: 0.7656\n",
      "Epoch 394/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.7708 - val_loss: 0.4953 - val_accuracy: 0.7656\n",
      "Epoch 395/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7708 - val_loss: 0.4952 - val_accuracy: 0.7656\n",
      "Epoch 396/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7708 - val_loss: 0.4952 - val_accuracy: 0.7656\n",
      "Epoch 397/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7708 - val_loss: 0.4951 - val_accuracy: 0.7656\n",
      "Epoch 398/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7708 - val_loss: 0.4951 - val_accuracy: 0.7656\n",
      "Epoch 399/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7708 - val_loss: 0.4950 - val_accuracy: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.7691 - val_loss: 0.4950 - val_accuracy: 0.7656\n",
      "Epoch 401/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7691 - val_loss: 0.4949 - val_accuracy: 0.7656\n",
      "Epoch 402/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7691 - val_loss: 0.4949 - val_accuracy: 0.7656\n",
      "Epoch 403/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7691 - val_loss: 0.4948 - val_accuracy: 0.7656\n",
      "Epoch 404/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7691 - val_loss: 0.4948 - val_accuracy: 0.7656\n",
      "Epoch 405/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7691 - val_loss: 0.4947 - val_accuracy: 0.7656\n",
      "Epoch 406/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.7691 - val_loss: 0.4947 - val_accuracy: 0.7656\n",
      "Epoch 407/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7691 - val_loss: 0.4946 - val_accuracy: 0.7656\n",
      "Epoch 408/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7691 - val_loss: 0.4946 - val_accuracy: 0.7656\n",
      "Epoch 409/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7691 - val_loss: 0.4945 - val_accuracy: 0.7656\n",
      "Epoch 410/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.7691 - val_loss: 0.4945 - val_accuracy: 0.7656\n",
      "Epoch 411/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7708 - val_loss: 0.4944 - val_accuracy: 0.7656\n",
      "Epoch 412/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7691 - val_loss: 0.4944 - val_accuracy: 0.7708\n",
      "Epoch 413/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7674 - val_loss: 0.4944 - val_accuracy: 0.7708\n",
      "Epoch 414/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7674 - val_loss: 0.4943 - val_accuracy: 0.7708\n",
      "Epoch 415/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7726 - val_loss: 0.4943 - val_accuracy: 0.7708\n",
      "Epoch 416/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7691 - val_loss: 0.4942 - val_accuracy: 0.7708\n",
      "Epoch 417/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7708 - val_loss: 0.4942 - val_accuracy: 0.7708\n",
      "Epoch 418/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7708 - val_loss: 0.4941 - val_accuracy: 0.7708\n",
      "Epoch 419/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7691 - val_loss: 0.4941 - val_accuracy: 0.7708\n",
      "Epoch 420/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.7708 - val_loss: 0.4940 - val_accuracy: 0.7708\n",
      "Epoch 421/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4790 - accuracy: 0.7691 - val_loss: 0.4940 - val_accuracy: 0.7656\n",
      "Epoch 422/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7691 - val_loss: 0.4940 - val_accuracy: 0.7656\n",
      "Epoch 423/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.7691 - val_loss: 0.4939 - val_accuracy: 0.7656\n",
      "Epoch 424/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7691 - val_loss: 0.4939 - val_accuracy: 0.7656\n",
      "Epoch 425/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7708 - val_loss: 0.4938 - val_accuracy: 0.7656\n",
      "Epoch 426/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7691 - val_loss: 0.4938 - val_accuracy: 0.7656\n",
      "Epoch 427/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7691 - val_loss: 0.4937 - val_accuracy: 0.7656\n",
      "Epoch 428/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7691 - val_loss: 0.4937 - val_accuracy: 0.7656\n",
      "Epoch 429/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7691 - val_loss: 0.4937 - val_accuracy: 0.7656\n",
      "Epoch 430/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7708 - val_loss: 0.4936 - val_accuracy: 0.7656\n",
      "Epoch 431/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7691 - val_loss: 0.4936 - val_accuracy: 0.7656\n",
      "Epoch 432/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7691 - val_loss: 0.4935 - val_accuracy: 0.7656\n",
      "Epoch 433/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7691 - val_loss: 0.4935 - val_accuracy: 0.7656\n",
      "Epoch 434/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7691 - val_loss: 0.4935 - val_accuracy: 0.7656\n",
      "Epoch 435/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.7708 - val_loss: 0.4934 - val_accuracy: 0.7656\n",
      "Epoch 436/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7691 - val_loss: 0.4934 - val_accuracy: 0.7708\n",
      "Epoch 437/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.7691 - val_loss: 0.4933 - val_accuracy: 0.7708\n",
      "Epoch 438/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7691 - val_loss: 0.4933 - val_accuracy: 0.7708\n",
      "Epoch 439/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7691 - val_loss: 0.4933 - val_accuracy: 0.7708\n",
      "Epoch 440/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7691 - val_loss: 0.4932 - val_accuracy: 0.7708\n",
      "Epoch 441/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.7691 - val_loss: 0.4932 - val_accuracy: 0.7708\n",
      "Epoch 442/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7691 - val_loss: 0.4931 - val_accuracy: 0.7708\n",
      "Epoch 443/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7691 - val_loss: 0.4931 - val_accuracy: 0.7708\n",
      "Epoch 444/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.7691 - val_loss: 0.4931 - val_accuracy: 0.7708\n",
      "Epoch 445/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7691 - val_loss: 0.4930 - val_accuracy: 0.7708\n",
      "Epoch 446/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7674 - val_loss: 0.4930 - val_accuracy: 0.7708\n",
      "Epoch 447/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.7674 - val_loss: 0.4929 - val_accuracy: 0.7708\n",
      "Epoch 448/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7674 - val_loss: 0.4929 - val_accuracy: 0.7708\n",
      "Epoch 449/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7674 - val_loss: 0.4929 - val_accuracy: 0.7708\n",
      "Epoch 450/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7674 - val_loss: 0.4928 - val_accuracy: 0.7708\n",
      "Epoch 451/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.7674 - val_loss: 0.4928 - val_accuracy: 0.7708\n",
      "Epoch 452/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7674 - val_loss: 0.4928 - val_accuracy: 0.7708\n",
      "Epoch 453/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.7674 - val_loss: 0.4927 - val_accuracy: 0.7708\n",
      "Epoch 454/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.7674 - val_loss: 0.4927 - val_accuracy: 0.7708\n",
      "Epoch 455/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4764 - accuracy: 0.7674 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
      "Epoch 456/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7674 - val_loss: 0.4926 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7674 - val_loss: 0.4926 - val_accuracy: 0.7708\n",
      "Epoch 458/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7691 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
      "Epoch 459/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.7691 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
      "Epoch 460/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.7691 - val_loss: 0.4925 - val_accuracy: 0.7708\n",
      "Epoch 461/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.7691 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
      "Epoch 462/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.7691 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
      "Epoch 463/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7691 - val_loss: 0.4924 - val_accuracy: 0.7708\n",
      "Epoch 464/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7691 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
      "Epoch 465/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7691 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
      "Epoch 466/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7691 - val_loss: 0.4923 - val_accuracy: 0.7708\n",
      "Epoch 467/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.7691 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
      "Epoch 468/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.7691 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
      "Epoch 469/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7691 - val_loss: 0.4922 - val_accuracy: 0.7708\n",
      "Epoch 470/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7691 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
      "Epoch 471/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7691 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
      "Epoch 472/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7691 - val_loss: 0.4921 - val_accuracy: 0.7708\n",
      "Epoch 473/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7674 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
      "Epoch 474/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.7691 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
      "Epoch 475/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.7691 - val_loss: 0.4920 - val_accuracy: 0.7708\n",
      "Epoch 476/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7691 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
      "Epoch 477/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7691 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
      "Epoch 478/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7691 - val_loss: 0.4919 - val_accuracy: 0.7708\n",
      "Epoch 479/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7691 - val_loss: 0.4918 - val_accuracy: 0.7708\n",
      "Epoch 480/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7691 - val_loss: 0.4918 - val_accuracy: 0.7708\n",
      "Epoch 481/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7691 - val_loss: 0.4918 - val_accuracy: 0.7708\n",
      "Epoch 482/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7691 - val_loss: 0.4917 - val_accuracy: 0.7708\n",
      "Epoch 483/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.7691 - val_loss: 0.4917 - val_accuracy: 0.7708\n",
      "Epoch 484/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4744 - accuracy: 0.7708 - val_loss: 0.4917 - val_accuracy: 0.7708\n",
      "Epoch 485/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4744 - accuracy: 0.7708 - val_loss: 0.4916 - val_accuracy: 0.7708\n",
      "Epoch 486/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7691 - val_loss: 0.4916 - val_accuracy: 0.7708\n",
      "Epoch 487/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.7708 - val_loss: 0.4916 - val_accuracy: 0.7708\n",
      "Epoch 488/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.7708 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 489/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7708 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 490/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7708 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 491/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.7691 - val_loss: 0.4915 - val_accuracy: 0.7708\n",
      "Epoch 492/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7708 - val_loss: 0.4914 - val_accuracy: 0.7708\n",
      "Epoch 493/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.7708 - val_loss: 0.4914 - val_accuracy: 0.7708\n",
      "Epoch 494/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.7708 - val_loss: 0.4914 - val_accuracy: 0.7708\n",
      "Epoch 495/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7691 - val_loss: 0.4913 - val_accuracy: 0.7708\n",
      "Epoch 496/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.7708 - val_loss: 0.4913 - val_accuracy: 0.7708\n",
      "Epoch 497/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.7691 - val_loss: 0.4913 - val_accuracy: 0.7708\n",
      "Epoch 498/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.7691 - val_loss: 0.4913 - val_accuracy: 0.7708\n",
      "Epoch 499/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.7708 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 500/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7691 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 501/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7691 - val_loss: 0.4912 - val_accuracy: 0.7708\n",
      "Epoch 502/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.7691 - val_loss: 0.4911 - val_accuracy: 0.7708\n",
      "Epoch 503/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.7691 - val_loss: 0.4911 - val_accuracy: 0.7708\n",
      "Epoch 504/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.7691 - val_loss: 0.4911 - val_accuracy: 0.7708\n",
      "Epoch 505/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7691 - val_loss: 0.4911 - val_accuracy: 0.7708\n",
      "Epoch 506/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.7691 - val_loss: 0.4910 - val_accuracy: 0.7708\n",
      "Epoch 507/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.7691 - val_loss: 0.4910 - val_accuracy: 0.7708\n",
      "Epoch 508/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7691 - val_loss: 0.4910 - val_accuracy: 0.7708\n",
      "Epoch 509/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7691 - val_loss: 0.4909 - val_accuracy: 0.7708\n",
      "Epoch 510/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4728 - accuracy: 0.7674 - val_loss: 0.4909 - val_accuracy: 0.7708\n",
      "Epoch 511/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.7691 - val_loss: 0.4909 - val_accuracy: 0.7708\n",
      "Epoch 512/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.7691 - val_loss: 0.4909 - val_accuracy: 0.7708\n",
      "Epoch 513/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7691 - val_loss: 0.4908 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7691 - val_loss: 0.4908 - val_accuracy: 0.7708\n",
      "Epoch 515/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7674 - val_loss: 0.4908 - val_accuracy: 0.7708\n",
      "Epoch 516/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7674 - val_loss: 0.4908 - val_accuracy: 0.7708\n",
      "Epoch 517/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7674 - val_loss: 0.4907 - val_accuracy: 0.7708\n",
      "Epoch 518/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7691 - val_loss: 0.4907 - val_accuracy: 0.7708\n",
      "Epoch 519/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.7674 - val_loss: 0.4907 - val_accuracy: 0.7708\n",
      "Epoch 520/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7674 - val_loss: 0.4907 - val_accuracy: 0.7708\n",
      "Epoch 521/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7674 - val_loss: 0.4906 - val_accuracy: 0.7708\n",
      "Epoch 522/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4721 - accuracy: 0.7674 - val_loss: 0.4906 - val_accuracy: 0.7708\n",
      "Epoch 523/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4721 - accuracy: 0.7691 - val_loss: 0.4906 - val_accuracy: 0.7708\n",
      "Epoch 524/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.7691 - val_loss: 0.4906 - val_accuracy: 0.7708\n",
      "Epoch 525/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.7674 - val_loss: 0.4905 - val_accuracy: 0.7708\n",
      "Epoch 526/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7691 - val_loss: 0.4905 - val_accuracy: 0.7708\n",
      "Epoch 527/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7691 - val_loss: 0.4905 - val_accuracy: 0.7708\n",
      "Epoch 528/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7691 - val_loss: 0.4905 - val_accuracy: 0.7708\n",
      "Epoch 529/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.7691 - val_loss: 0.4904 - val_accuracy: 0.7708\n",
      "Epoch 530/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.7691 - val_loss: 0.4904 - val_accuracy: 0.7708\n",
      "Epoch 531/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.7691 - val_loss: 0.4904 - val_accuracy: 0.7708\n",
      "Epoch 532/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.7691 - val_loss: 0.4904 - val_accuracy: 0.7708\n",
      "Epoch 533/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4715 - accuracy: 0.7691 - val_loss: 0.4903 - val_accuracy: 0.7708\n",
      "Epoch 534/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4715 - accuracy: 0.7708 - val_loss: 0.4903 - val_accuracy: 0.7708\n",
      "Epoch 535/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7691 - val_loss: 0.4903 - val_accuracy: 0.7708\n",
      "Epoch 536/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7691 - val_loss: 0.4903 - val_accuracy: 0.7708\n",
      "Epoch 537/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7691 - val_loss: 0.4902 - val_accuracy: 0.7708\n",
      "Epoch 538/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.7691 - val_loss: 0.4902 - val_accuracy: 0.7708\n",
      "Epoch 539/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.7691 - val_loss: 0.4902 - val_accuracy: 0.7708\n",
      "Epoch 540/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4711 - accuracy: 0.7691 - val_loss: 0.4902 - val_accuracy: 0.7708\n",
      "Epoch 541/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4711 - accuracy: 0.7691 - val_loss: 0.4902 - val_accuracy: 0.7708\n",
      "Epoch 542/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.7691 - val_loss: 0.4901 - val_accuracy: 0.7708\n",
      "Epoch 543/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7691 - val_loss: 0.4901 - val_accuracy: 0.7708\n",
      "Epoch 544/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7708 - val_loss: 0.4901 - val_accuracy: 0.7708\n",
      "Epoch 545/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7691 - val_loss: 0.4901 - val_accuracy: 0.7708\n",
      "Epoch 546/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.7708 - val_loss: 0.4900 - val_accuracy: 0.7708\n",
      "Epoch 547/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.7708 - val_loss: 0.4900 - val_accuracy: 0.7708\n",
      "Epoch 548/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.7708 - val_loss: 0.4900 - val_accuracy: 0.7708\n",
      "Epoch 549/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.7708 - val_loss: 0.4900 - val_accuracy: 0.7708\n",
      "Epoch 550/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.7708 - val_loss: 0.4900 - val_accuracy: 0.7708\n",
      "Epoch 551/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.7691 - val_loss: 0.4899 - val_accuracy: 0.7708\n",
      "Epoch 552/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.7708 - val_loss: 0.4899 - val_accuracy: 0.7708\n",
      "Epoch 553/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.7691 - val_loss: 0.4899 - val_accuracy: 0.7708\n",
      "Epoch 554/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.7691 - val_loss: 0.4899 - val_accuracy: 0.7708\n",
      "Epoch 555/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.7691 - val_loss: 0.4899 - val_accuracy: 0.7708\n",
      "Epoch 556/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.7691 - val_loss: 0.4898 - val_accuracy: 0.7708\n",
      "Epoch 557/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7708 - val_loss: 0.4898 - val_accuracy: 0.7708\n",
      "Epoch 558/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7691 - val_loss: 0.4898 - val_accuracy: 0.7708\n",
      "Epoch 559/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7691 - val_loss: 0.4898 - val_accuracy: 0.7708\n",
      "Epoch 560/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7708 - val_loss: 0.4898 - val_accuracy: 0.7708\n",
      "Epoch 561/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.7691 - val_loss: 0.4897 - val_accuracy: 0.7708\n",
      "Epoch 562/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.7708 - val_loss: 0.4897 - val_accuracy: 0.7708\n",
      "Epoch 563/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.7708 - val_loss: 0.4897 - val_accuracy: 0.7708\n",
      "Epoch 564/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.7691 - val_loss: 0.4897 - val_accuracy: 0.7708\n",
      "Epoch 565/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.7691 - val_loss: 0.4897 - val_accuracy: 0.7708\n",
      "Epoch 566/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.7691 - val_loss: 0.4896 - val_accuracy: 0.7708\n",
      "Epoch 567/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.7708 - val_loss: 0.4896 - val_accuracy: 0.7708\n",
      "Epoch 568/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7708 - val_loss: 0.4896 - val_accuracy: 0.7708\n",
      "Epoch 569/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7691 - val_loss: 0.4896 - val_accuracy: 0.7708\n",
      "Epoch 570/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7708 - val_loss: 0.4896 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 571/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.7726 - val_loss: 0.4895 - val_accuracy: 0.7708\n",
      "Epoch 572/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4695 - accuracy: 0.7708 - val_loss: 0.4895 - val_accuracy: 0.7708\n",
      "Epoch 573/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4695 - accuracy: 0.7708 - val_loss: 0.4895 - val_accuracy: 0.7708\n",
      "Epoch 574/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7726 - val_loss: 0.4895 - val_accuracy: 0.7708\n",
      "Epoch 575/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7708 - val_loss: 0.4895 - val_accuracy: 0.7708\n",
      "Epoch 576/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.7726 - val_loss: 0.4894 - val_accuracy: 0.7708\n",
      "Epoch 577/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.7708 - val_loss: 0.4894 - val_accuracy: 0.7708\n",
      "Epoch 578/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.7726 - val_loss: 0.4894 - val_accuracy: 0.7708\n",
      "Epoch 579/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.7708 - val_loss: 0.4894 - val_accuracy: 0.7708\n",
      "Epoch 580/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.7726 - val_loss: 0.4894 - val_accuracy: 0.7708\n",
      "Epoch 581/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.7726 - val_loss: 0.4894 - val_accuracy: 0.7708\n",
      "Epoch 582/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7708 - val_loss: 0.4893 - val_accuracy: 0.7708\n",
      "Epoch 583/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7708 - val_loss: 0.4893 - val_accuracy: 0.7708\n",
      "Epoch 584/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7708 - val_loss: 0.4893 - val_accuracy: 0.7708\n",
      "Epoch 585/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.7708 - val_loss: 0.4893 - val_accuracy: 0.7708\n",
      "Epoch 586/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7726 - val_loss: 0.4893 - val_accuracy: 0.7708\n",
      "Epoch 587/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7708 - val_loss: 0.4893 - val_accuracy: 0.7708\n",
      "Epoch 588/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7708 - val_loss: 0.4892 - val_accuracy: 0.7708\n",
      "Epoch 589/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7726 - val_loss: 0.4892 - val_accuracy: 0.7708\n",
      "Epoch 590/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7708 - val_loss: 0.4892 - val_accuracy: 0.7708\n",
      "Epoch 591/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7726 - val_loss: 0.4892 - val_accuracy: 0.7708\n",
      "Epoch 592/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7708 - val_loss: 0.4892 - val_accuracy: 0.7708\n",
      "Epoch 593/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7726 - val_loss: 0.4892 - val_accuracy: 0.7708\n",
      "Epoch 594/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7726 - val_loss: 0.4891 - val_accuracy: 0.7708\n",
      "Epoch 595/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7726 - val_loss: 0.4891 - val_accuracy: 0.7708\n",
      "Epoch 596/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7708 - val_loss: 0.4891 - val_accuracy: 0.7708\n",
      "Epoch 597/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7726 - val_loss: 0.4891 - val_accuracy: 0.7708\n",
      "Epoch 598/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.7726 - val_loss: 0.4891 - val_accuracy: 0.7708\n",
      "Epoch 599/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.7743 - val_loss: 0.4891 - val_accuracy: 0.7708\n",
      "Epoch 600/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.7743 - val_loss: 0.4890 - val_accuracy: 0.7708\n",
      "Epoch 601/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.7743 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 602/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.7743 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 603/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.7743 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 604/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7743 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 605/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7743 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 606/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7726 - val_loss: 0.4890 - val_accuracy: 0.7656\n",
      "Epoch 607/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7708 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 608/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7708 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 609/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.7708 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 610/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.7726 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 611/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7708 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 612/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7726 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 613/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7743 - val_loss: 0.4889 - val_accuracy: 0.7656\n",
      "Epoch 614/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.7708 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 615/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.7726 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 616/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7726 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 617/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7708 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 618/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7708 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 619/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.7726 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 620/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.7743 - val_loss: 0.4888 - val_accuracy: 0.7656\n",
      "Epoch 621/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 622/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 623/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 624/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 625/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 626/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 627/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.7743 - val_loss: 0.4887 - val_accuracy: 0.7656\n",
      "Epoch 629/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7726 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 630/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7743 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 631/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7726 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 632/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7743 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 633/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7743 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 634/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7743 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 635/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7726 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 636/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.7743 - val_loss: 0.4886 - val_accuracy: 0.7656\n",
      "Epoch 637/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 638/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.7726 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 639/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 640/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 641/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 642/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 643/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 644/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.7743 - val_loss: 0.4885 - val_accuracy: 0.7656\n",
      "Epoch 645/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 646/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 647/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 648/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 649/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7726 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 650/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 651/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 652/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 653/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7743 - val_loss: 0.4884 - val_accuracy: 0.7656\n",
      "Epoch 654/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7743 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 655/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7743 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 656/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 657/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 658/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 659/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 660/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 661/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 662/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 663/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7726 - val_loss: 0.4883 - val_accuracy: 0.7656\n",
      "Epoch 664/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7726 - val_loss: 0.4882 - val_accuracy: 0.7656\n",
      "Epoch 665/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7726 - val_loss: 0.4882 - val_accuracy: 0.7656\n",
      "Epoch 666/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.7743 - val_loss: 0.4882 - val_accuracy: 0.7656\n",
      "Epoch 667/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.7726 - val_loss: 0.4882 - val_accuracy: 0.7656\n",
      "Epoch 668/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.7726 - val_loss: 0.4882 - val_accuracy: 0.7656\n",
      "Epoch 669/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7726 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 670/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7726 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 671/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7743 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 672/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.7743 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 673/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.7743 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 674/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.7760 - val_loss: 0.4882 - val_accuracy: 0.7604\n",
      "Epoch 675/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7743 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 676/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 677/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 678/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 679/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 680/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 681/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 682/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 683/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 684/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7760 - val_loss: 0.4881 - val_accuracy: 0.7604\n",
      "Epoch 686/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 687/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 688/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 689/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 690/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 691/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 692/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 693/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 694/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 695/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 696/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 697/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 698/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.7760 - val_loss: 0.4880 - val_accuracy: 0.7604\n",
      "Epoch 699/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 700/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 701/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 702/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 703/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 704/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 705/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 706/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 707/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 708/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 709/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 710/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7778 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 711/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 712/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7760 - val_loss: 0.4879 - val_accuracy: 0.7604\n",
      "Epoch 713/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7760 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 714/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7778 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 715/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7778 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 716/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7760 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 717/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7760 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 718/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7778 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 719/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7760 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 720/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7778 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 721/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7778 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 722/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7760 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 723/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7778 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 724/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7795 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 725/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7795 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 726/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7795 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 727/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7795 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 728/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7795 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 729/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7795 - val_loss: 0.4878 - val_accuracy: 0.7604\n",
      "Epoch 730/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 731/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 732/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 733/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 734/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 735/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 736/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 737/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 738/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 739/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 740/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 741/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 742/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7604\n",
      "Epoch 743/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 744/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 745/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 746/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 747/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 748/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 749/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7795 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 750/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 751/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 752/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 753/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 754/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 755/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 756/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 757/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 758/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 759/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 760/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 761/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 762/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 763/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 764/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 765/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 766/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 767/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 768/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 769/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 770/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 771/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7778 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 772/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7760 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 773/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7778 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 774/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 775/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7778 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 776/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7778 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 777/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7760 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 778/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 779/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7760 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 780/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 781/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 782/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 783/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7760 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 784/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 785/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 786/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 787/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 788/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 789/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7760 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 790/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 791/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 792/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 793/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 794/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 795/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 796/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 797/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 798/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 800/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 801/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 802/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 803/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 804/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 805/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 806/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 807/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 808/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 809/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 810/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 811/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 812/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 813/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 814/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 815/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 816/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 817/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 818/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 819/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 820/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 821/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 822/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7795 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 823/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 824/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 825/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 826/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 827/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 828/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 829/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 830/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 831/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 832/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 833/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 834/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 835/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 836/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 837/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 838/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 839/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 840/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 841/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 842/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 843/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 844/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 845/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 846/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 847/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 848/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 849/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 850/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 851/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 852/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 853/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 854/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 855/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 856/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 857/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 858/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 859/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 860/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 861/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 862/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 863/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 864/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 865/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 866/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 867/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 868/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 869/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 870/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 871/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 872/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 873/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 874/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 875/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 876/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 877/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 878/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 879/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 880/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 881/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 882/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 883/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 884/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 885/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 886/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 887/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 888/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 889/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 890/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 891/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 892/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 893/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 894/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 895/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 896/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 897/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 898/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 899/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 900/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7604\n",
      "Epoch 901/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 902/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 903/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 904/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 905/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 906/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 907/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 908/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 909/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 910/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 911/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7795 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 912/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 913/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 914/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7795 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 915/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7795 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 916/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7778 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 917/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7795 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 918/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 919/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 920/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 921/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 922/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 923/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 924/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 925/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 926/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 927/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7812 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 928/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 929/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 930/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 931/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 932/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 933/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7812 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 934/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 935/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7812 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 936/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7812 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 937/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7812 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 938/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 939/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 940/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 941/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 942/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 943/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 944/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 945/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 946/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 947/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 948/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 949/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 950/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 951/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 952/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 953/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 954/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 955/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7778 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 956/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 957/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 958/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 959/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 960/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 961/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 962/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 963/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 964/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 965/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 966/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 967/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 968/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 969/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 971/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 972/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 973/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 974/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 975/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 976/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 977/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 978/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 979/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 980/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 981/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 982/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 983/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 984/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 985/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 986/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 987/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 988/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 989/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 990/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 991/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 992/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 993/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 994/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 995/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 996/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 997/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 998/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 999/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n",
      "Epoch 1000/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7795 - val_loss: 0.4876 - val_accuracy: 0.7604\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b25b4154f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "For this exercise, do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(6, input_shape=(8,), activation=\"relu\"))\n",
    "model_2.add(Dense(6,  activation=\"relu\"))\n",
    "model_2.add(Dense(1, activation=\"sigmoid\"))\n",
    "#  Adam, SGD, RMSprop\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hist_2.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(run_hist_2.history[\"loss\"])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(range(n), (run_hist_2.history[\"loss\"]),'r.', label=\"Train Loss\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_loss\"]),'b.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "ax.set_title('Loss over iterations')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(range(n), (run_hist_2.history[\"accuracy\"]),'r.', label=\"Train Acc\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_accuracy\"]),'b.', label=\"Validation Acc\")\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('Accuracy over iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_nn_2 = model_2.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_2 = model_2.predict(X_test_norm)\n",
    "print('')\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN-2')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
